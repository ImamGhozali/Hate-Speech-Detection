{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "id_stopword_dict = pd.read_csv('/Users/imamghozali/Documents/Master/Semester1/RPL/Data/stopwordbahasa.csv', header=None)\n",
    "id_stopword_dict = id_stopword_dict.rename(columns={0: 'stopword'})\n",
    "\n",
    "# print(id_stopword_dict)\n",
    "data = pd.read_csv(\n",
    "    \"/Users/imamghozali/Documents/Master/Semester1/RPL/Data/re_dataset.csv\", error_bad_lines=False, encoding='latin-1')\n",
    "data1 = data.drop(['Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race',\n",
    "                  'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong'], axis=1)\n",
    "\n",
    "alay_dict = pd.read_csv('/Users/imamghozali/Documents/Master/Semester1/RPL/Data/new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', \n",
    "                                      1: 'replacement'})\n",
    "# print(data1.values[0][0])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mt/m90qwp7n661bp7scgs92_n300000gn/T/ipykernel_85218/2063137788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mid_stopword_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/imamghozali/Documents/Master/Semester1/RPL/Data/stopwordbahasa.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot('HS',data=data1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    # Remove every URL\n",
    "    text = re.sub(\n",
    "        '((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)  # Remove every '\\n'\n",
    "    text = re.sub('\\r', ' ', text)  # Remove every '\\r'\n",
    "    text = re.sub('(?i)rt', ' ', text)  # Remove every retweet symbol\n",
    "    text = re.sub('@[^\\s]+[ \\t]', '', text)  # Remove every username\n",
    "    text = re.sub('(?i)user', '', text)  # Remove every username\n",
    "    text = re.sub('(?i)url', ' ', text)  # Remove every url\n",
    "    text = re.sub(r'\\\\x..', ' ', text)  # Remove every emoji\n",
    "    text = re.sub('  +', ' ', text)  # Remove extra spaces\n",
    "    # Remove characters repeating more than twice\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "\n",
    "\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "\n",
    "def remove_stopword(text):\n",
    "    text = ' '.join(\n",
    "        ['' if word in id_stopword_dict.stopword.values else word for word in text.split(' ')])\n",
    "    text = re.sub('  +', ' ', text)  # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = normalize_alay(text)\n",
    "    text = remove_stopword(text)\n",
    "    text = stemming(text)\n",
    "    text = nltk.tokenize.word_tokenize(text)\n",
    "    ordered_tokens = set()\n",
    "    result = []\n",
    "    # filter duplicate words\n",
    "    # for word in text:\n",
    "    #     if word not in ordered_tokens:\n",
    "    #         ordered_tokens.add(word)\n",
    "    #         result.append(word)\n",
    "    # text = result      \n",
    "    return text\n",
    "\n",
    "print(preprocess(\"@saidaqil Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"))\n",
    "print(preprocess(\"@saidaqil apa maksud pernyataan. Anda? Apa anda mengaminkan kriminalisasi Ahok?? Hati2 jg dg mulutmu pak!\"))\n",
    "# data1 = data1.drop(['HS'], axis=1)\n",
    "# result = []\n",
    "# for item in data1.iloc[:, 0]:\n",
    "#     # for num in item:    \n",
    "#         # print(preprocess(item))\n",
    "#         result.append(preprocess(item))\n",
    "\n",
    "# print(result[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x=data1['Tweet']\n",
    "y=data1['HS']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42,test_size=0.2)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import string\n",
    "\n",
    "# Create an iterator object that returns words properly formatted for Word2Vec training\n",
    "class TweetIterator:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        # The text contains a stupid special character, hence the + '–'\n",
    "        self.translator = str.maketrans('', '', string.punctuation + '–')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tweet in self.dataset:\n",
    "            # Make all characters lower-case\n",
    "            tweet = tweet.lower()\n",
    "            sentence= preprocess(tweet)\n",
    "            words = [w for w in sentence if w != '']\n",
    "            # for sentence in tweet.split('.')[:-1]:\n",
    "            #     # Clean string of all punctuation\n",
    "            #     # sentence = sentence.translate(self.translator)\n",
    "            #     sentence= preprocess(sentence)\n",
    "            #     words = [w for w in sentence]\n",
    "\n",
    "            yield words\n",
    "\n",
    "sentences = TweetIterator(x_train)\n",
    "# print(list(sentences))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gensim\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(sentences, window=2, min_count=10, workers=8)\n",
    "\n",
    "# Retrieve the weights from the model. This is used for initializing the weights\n",
    "# in a Keras Embedding layer later\n",
    "w2v_weights = w2v_model.wv.vectors\n",
    "vocab_size, embedding_size = w2v_weights.shape\n",
    "\n",
    "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(w2v_model.wv.most_similar('anjir', topn=3))\n",
    "\n",
    "def word2token(word):\n",
    "    try:\n",
    "        return w2v_model.wv.key_to_index[word]\n",
    "    # If word is not in index return 0. I realize this means that this\n",
    "    # is the same as the word of index 0 (i.e. most frequent word), but 0s\n",
    "    # will be padded later anyway by the embedding layer (which also\n",
    "    # seems dirty but I couldn't find a better solution right now)\n",
    "    except KeyError:\n",
    "        return 0\n",
    "def token2word(token):\n",
    "    return w2v_model.wv.index2word[token]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "categories, ccount = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Using a figsize this big seems dirty. It's the only way I figured to make\n",
    "# the x labels not overlap. I suck at plotting, sorry\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title(\"Dataset Category Distribution\")\n",
    "plt.bar(categories, ccount, align='center')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import everything that will be used\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import string\n",
    "# Create an iterator that formats data from the dataset proper for\n",
    "# LSTM training\n",
    "\n",
    "# Sequences will be padded or truncated to this length\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "# Samples of categories with less than this number of samples will be ignored\n",
    "DROP_THRESHOLD = 100\n",
    "\n",
    "class SequenceIterator:\n",
    "    def __init__(self, dataset_x,dataset_y, drop_threshold, seq_length):\n",
    "        self.dataset_x = dataset_x\n",
    "        self.dataset_y = dataset_y\n",
    "        self.translator = str.maketrans('', '', string.punctuation + '–')\n",
    "        self.categories, self.ccount = np.unique(dataset_y, return_counts=True)\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Samples of categories with less than this number of samples will be ignored\n",
    "        self.drop_categos = []\n",
    "        for cat, count in zip(self.categories, self.ccount):\n",
    "            if count < drop_threshold:\n",
    "                self.drop_categos.append(cat)\n",
    "        \n",
    "        # Remaining categories\n",
    "        self.categories = np.setdiff1d(self.categories, self.drop_categos)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for tweet, cat in zip(self.dataset_x, self.dataset_y):\n",
    "            if cat in self.drop_categos:\n",
    "                continue\n",
    "            \n",
    "            # Make all characters lower-case\n",
    "            tweet = tweet.lower()\n",
    "            \n",
    "            # Clean string of all punctuation\n",
    "            # tweet = tweet.translate(self.translator)\n",
    "            tweet= preprocess(tweet)\n",
    "\n",
    "\n",
    "            words = np.array([word2token(w) for w in tweet[:self.seq_length] if w != ''])\n",
    "                                \n",
    "            yield (words, cat)\n",
    "\n",
    "sequences = SequenceIterator(x_train,y_train, DROP_THRESHOLD, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Used for generating the labels in the set\n",
    "cat_dict = {k: v for k, v in zip(sequences.categories, range(len(sequences.categories)))}\n",
    "\n",
    "set_x = []\n",
    "set_y = []\n",
    "for w, c in sequences:\n",
    "    set_x.append(w)\n",
    "    set_y.append(cat_dict[c])\n",
    "    \n",
    "# Padding sequences with 0.\n",
    "set_x = pad_sequences(set_x, maxlen=MAX_SEQUENCE_LENGTH, padding='pre', value=0)\n",
    "set_y = np.array(set_y)\n",
    "\n",
    "print(set_x.shape)\n",
    "print(set_y.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import random\n",
    "\n",
    "# VALID_PER = 0.20 # Percentage of the whole set that will be separated for validation\n",
    "\n",
    "# total_samples = set_x.shape[0]\n",
    "# n_val = int(VALID_PER * total_samples)\n",
    "# n_train = total_samples - n_val\n",
    "\n",
    "# random_i = random.sample(range(total_samples), total_samples)\n",
    "# train_x = set_x[random_i[:n_train]]\n",
    "# train_y = set_y[random_i[:n_train]]\n",
    "# val_x = set_x[random_i[n_train:n_train+n_val]]\n",
    "# val_y = set_y[random_i[n_train:n_train+n_val]]\n",
    "\n",
    "# # print(train_x)\n",
    "\n",
    "# print(\"Train Shapes - X: {} - Y: {}\".format(train_x.shape, train_y.shape))\n",
    "# print(\"Val Shapes - X: {} - Y: {}\".format(val_x.shape, val_y.shape))\n",
    "\n",
    "# # Let's look at the distribution of categories in both sets\n",
    "# categories, ccount = np.unique(train_y, return_counts=True)\n",
    "# plt.figure(figsize=(16, 8))\n",
    "# plt.title(\"Training Set - Category Distribution\")\n",
    "# plt.xticks(range(len(categories)), cat_dict.keys())\n",
    "# plt.bar(categories, ccount, align='center')\n",
    "# plt.show()\n",
    "\n",
    "# categories, ccount = np.unique(val_y, return_counts=True)\n",
    "# plt.figure(figsize=(16, 8))\n",
    "# plt.title(\"Validation Set - Category Distribution\")\n",
    "# plt.xticks(range(len(categories)), cat_dict.keys())\n",
    "# plt.bar(categories, ccount, align='center')\n",
    "# plt.show()\n",
    "\n",
    "# n_categories = len(categories)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from time import time  # To time our operations\n",
    "\n",
    "# cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "\n",
    "# w2v_model = Word2Vec(min_count=20,\n",
    "#                      window=2,\n",
    "#                      vector_size=100,\n",
    "#                      sample=6e-5, \n",
    "#                      alpha=0.03, \n",
    "#                      min_alpha=0.001, \n",
    "#                      negative=20,\n",
    "#                      workers=cores-3)\n",
    "# t = time()\n",
    "\n",
    "# w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "# print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "# # t = time()\n",
    "\n",
    "# w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "# print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "# w2v_model.wv.most_similar(positive=[\"cowok\"])\n",
    "# print(data1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# # Keras Embedding layer with Word2Vec weights initialization\n",
    "# model.add(Embedding(input_dim=vocab_size,\n",
    "#                     output_dim=embedding_size,\n",
    "#                     weights=[w2v_weights],\n",
    "#                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                     mask_zero=True,\n",
    "#                     trainable=False))\n",
    "\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dense(n_categories, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_x, train_y, epochs=5, batch_size=64,\n",
    "#                     validation_data=(val_x, val_y), verbose=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "stop = EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    mode='max',\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "checkpoint= ModelCheckpoint(\n",
    "    filepath='./',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history=model.fit(set_x,set_y,batch_size=1024,epochs=50,\n",
    "          validation_split=0.2,callbacks=[stop,checkpoint])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sequences = SequenceIterator(x_test,y_test, DROP_THRESHOLD, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Used for generating the labels in the set\n",
    "cat_dict = {k: v for k, v in zip(sequences.categories, range(len(sequences.categories)))}\n",
    "\n",
    "set_x_test = []\n",
    "set_y_test = []\n",
    "for w, c in sequences:\n",
    "    set_x_test.append(w)\n",
    "    set_y_test.append(cat_dict[c])\n",
    "    \n",
    "# Padding sequences with 0.\n",
    "set_x_test = pad_sequences(set_x_test, maxlen=MAX_SEQUENCE_LENGTH, padding='pre', value=0)\n",
    "set_y_test = np.array(set_y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=MAX_SEQUENCE_LENGTH)\n",
    "# tokenizer.fit_on_texts(set_x_test)\n",
    "# test_sequences = tokenizer.texts_to_sequences(set_x_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(set_x_test,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "accr = model.evaluate(test_sequences_matrix,y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=MAX_SEQUENCE_LENGTH)\n",
    "# tokenizer.fit_on_texts(x_test)\n",
    "# test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "# test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# accr = model.evaluate(test_sequences_matrix,y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lstm_prediction=model.predict(test_sequences_matrix)\n",
    "res=[]\n",
    "for prediction in lstm_prediction:\n",
    "    if prediction[0]<0.5:\n",
    "        res.append(0)\n",
    "    else:\n",
    "        res.append(1)\n",
    "print(confusion_matrix(set_y_test,res))\n",
    "print (classification_report(set_y_test, res))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(set_x, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save(\"hate_model.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import keras\n",
    "load_model=keras.models.load_model(\"./hate_model.h5\")\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    load_tokenizer = pickle.load(handle)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TestIterator:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        # The text contains a stupid special character, hence the + '–'\n",
    "        self.translator = str.maketrans('', '', string.punctuation + '–')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tweet in self.dataset:\n",
    "            # Make all characters lower-case\n",
    "            tweet = tweet.lower()\n",
    "            tweet = preprocess(tweet)\n",
    "            words = [word2token(w)\n",
    "                             for w in tweet[:MAX_SEQUENCE_LENGTH] if w != '']\n",
    "\n",
    "            # words = [w for w in sentence if w != '']\n",
    "            # for sentence in tweet.split('.')[:-1]:\n",
    "            #     # Clean string of all punctuation\n",
    "            #     # sentence = sentence.translate(self.translator)\n",
    "            #     sentence= preprocess(sentence)\n",
    "            #     words = [w for w in sentence]\n",
    "\n",
    "            yield words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test = \"- disaat semua cowok berusaha melacak perhatian gue. loe lantas remehkan perhatian yg gue kasih khusus ke elo. basic elo cowok bego ! ! !'\"\n",
    "\n",
    "sequences = SequenceIterator(test,y_test, DROP_THRESHOLD, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Used for generating the labels in the set\n",
    "cat_dict = {k: v for k, v in zip(sequences.categories, range(len(sequences.categories)))}\n",
    "\n",
    "predict_x_test = []\n",
    "set_y_test = []\n",
    "for w, c in sequences:\n",
    "    predict_x_test.append(w)\n",
    "    set_y_test.append(cat_dict[c])\n",
    "    \n",
    "# Padding sequences with 0.\n",
    "predict_x_test = pad_sequences(predict_x_test, maxlen=MAX_SEQUENCE_LENGTH, padding='pre', value=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "test=[preprocess(test)]\n",
    "print(test)\n",
    "seq = tokenizer.texts_to_sequences(test)\n",
    "padded = sequence.pad_sequences(seq, maxlen=200)\n",
    "print(seq)\n",
    "pred = model.predict(padded)\n",
    "print(\"pred\", pred)\n",
    "if pred<0.5:\n",
    "    print(\"no hate\")\n",
    "else:\n",
    "    print(\"hate\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test = \"- disaat semua cowok berusaha melacak perhatian gue. loe lantas remehkan perhatian yg gue kasih khusus ke elo. basic elo cowok bego ! ! !'\"\n",
    "\n",
    "\n",
    "test=[preprocess(test)]\n",
    "# print(predict_x_test)\n",
    "# seq = tokenizer.texts_to_sequences(test)\n",
    "seq = TestIterator(test)\n",
    "padded = pad_sequences(seq, maxlen=200)\n",
    "pred = model.predict(padded)[0]\n",
    "print(\"pred\", pred)\n",
    "if pred<0.5:\n",
    "    print(\"no hate\")\n",
    "else:\n",
    "    print(\"hate\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lstm_prediction=model.predict(predict_x_test)\n",
    "res=[]\n",
    "for prediction in lstm_prediction:\n",
    "    if prediction[0]<0.5:\n",
    "        res.append(0)\n",
    "    else:\n",
    "        res.append(1)\n",
    "print(res)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}